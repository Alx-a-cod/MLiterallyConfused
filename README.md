*A Repository for ML univerisitarian courseworks & project that‚Äôs equal parts supervised learning and supervised chaos.*

---

### üìç Objective
The goal of this project is to develop a foundational understanding of **supervised learning algorithms** by implementing, analyzing, and comparing multiple models on a real-world dataset.

The chosen task is a **binary classification problem** ‚Äî predicting diabetes outcomes based on medical and demographic data. 

The dataset contains over **1,000 data points**, derived and expanded from the **[Diabetes Dataset on Kaggle](https://www.kaggle.com/datasets/saurabh00007/diabetescsv)**.

---

### üìç Project Structure

#### ‚§ø Part 1 ‚Äî Data Selection

- **Dataset:** Diabetes Dataset (Kaggle, see link above)  
- **Task:** Binary classification (Diabetic / Non-Diabetic)  
- **Number of samples:** 1,200 (originally 769, expanded for analysis)  

---

#### ‚§ø Part 2 ‚Äî Data Preprocessing

Steps include:

- Handling missing or noisy data  
- Normalizing or standardizing continuous features  
- Splitting data into **training**, **validation**, and **test** sets  
- Optional dimensionality reduction (e.g., PCA) or feature selection  

---

#### ‚§ø Part 3 ‚Äî Model Implementation and Training
The following supervised learning models are implemented using **Python** and **scikit-learn**:

| Model | Type | Notes |
|-------|------|-------|
| Gaussian Na√Øve Bayes | Classification | Simple probabilistic baseline |
| Logistic Regression | Binary classification | Core linear model for probabilities |
| Decision Tree | Classification | Interpretable model, useful for feature insights |
| Random Forest | Classification | Ensemble approach for improved stability |
| SVM (Linear & RBF Kernel) | Binary classification | Margin-based classifier for complex boundaries |

*(Softmax and Linear Regression excluded as this is a binary classification task.)*

---

#### ‚§ø Part 4 ‚Äî Evaluation
Models are compared using:
- **Accuracy**, **Precision**, **Recall**, **F1-Score**  
- **Confusion Matrix**  
- **ROC Curve** and **AUC**  
- **Training vs. Validation** performance  
- *(Optional)* computational cost and training time  

---

#### ‚§ø Part 5 ‚Äî Comparative Analysis
A technical report (PDF, prepared in **LaTeX**) includes:
- Model performance comparison and interpretation  
- Influence of model assumptions  
- Observations on overfitting and generalization  
- Visualizations: learning curves, decision boundaries, feature importance  

---

#### ‚§ø Deliverables
1. **Technical Report (PDF)** ‚Äî includes introduction, methodology, results, and conclusion.  
2. **Google Colab Notebook** ‚Äî clean, well-commented, and reproducible experiments.  

---

#### ‚§ø Tools & Libraries
- **Python 3.x**  
- **NumPy**, **Pandas**  
- **Matplotlib**, **Seaborn**  
- **Scikit-learn**  
- **Google Colab**  
- **LaTeX** (for the report)

---

##### üîñ License

Developed as part of an academic requirement. Licensing details TBD, but Apache 2.0 just to be on the safer side.

---
  
#### üë• Author(s)

[**Gyanluca**]   ‚ï∞‚îà‚û§ÀéÀäÀó (https://github.com/gyanluca)  
Me ‚Äî *still debugging life, one dataset at a time.*  
  
[![GitHub - Alx-a-cod, Author](https://img.shields.io/badge/author-Alx--a--cod-F2928D?logo=github)](https://github.com/Alx-a-cod)  [![GitHub - Gyanluca, Author](https://img.shields.io/badge/author-Gyanluca-4a74c2?logo=github)](https://github.com/gyanluca)
